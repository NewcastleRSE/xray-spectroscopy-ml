

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Library of Models &mdash; xanesnet 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=e031e9a9"></script>
      <script src="_static/doctools.js?v=888ff710"></script>
      <script src="_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Model Evaluation" href="evaluation.html" />
    <link rel="prev" title="Library of Descriptors" href="descriptor.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            xanesnet
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installing XANESNET</a></li>
<li class="toctree-l1"><a class="reference internal" href="running.html">Running XANESNET</a></li>
<li class="toctree-l1"><a class="reference internal" href="input.html">Input File</a></li>
<li class="toctree-l1"><a class="reference internal" href="descriptor.html">Library of Descriptors</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Library of Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#mlp">MLP</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cnn">CNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lstm">LSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ae-mlp">AE-MLP</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ae-cnn">AE-CNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#aegan-mlp">AEGAN-MLP</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">Model Evaluation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Project Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/NewcastleRSE/xray-spectroscopy-ml">XANESNET GitHub Repository</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitlab.com/team-xnet/xanesnet">XANESNET GitLab Repostiory</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitlab.com/team-xnet/training-sets">XANESNET Training Datasets</a></li>
<li class="toctree-l1"><a class="reference external" href="http://penfoldgroup.co.uk">Penfold Group</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitlab.com/team-xnet/xanesnet_keras">Original Keras Implementation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">xanesnet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Library of Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/model.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="library-of-models">
<h1>Library of Models<a class="headerlink" href="#library-of-models" title="Permalink to this heading"></a></h1>
<section id="mlp">
<h2>MLP<a class="headerlink" href="#mlp" title="Permalink to this heading"></a></h2>
<p>The Multi-Layer Perceptron (MLP) model is a feedforward artificial neural network.
In XANESNET, an MLP consists of an input layer, one or more hidden layers, and an output layer.
All layers are dense, meaning they are fully connected. Each hidden layer
comprises a linear layer, a dropout layer and activation function.</p>
<p>Users can define the number of hidden layers,
the dropout rate, and the type of activation function through the input file.
Additionally, the model considers a shrink rate which multiplicatively reduce the size of each
subsequent hidden layer, in order to
help in reducing computational complexity and controlling overfitting.</p>
<p><strong>Input file:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">type:</span> <span class="pre">mlp</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">params</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_size</span></code> (int): The size of the initial hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dropout</span></code> (float):  Dropout probability for the hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_hidden_layers</span></code> (int): Number of hidden layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shrink_rate</span></code> (float): Rate to reduce the hidden layer size multiplicatively.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">activation</span></code> (str): Type of activation function. Option: <em>relu</em> or <em>sigmoid</em> or <em>tanh</em> or <em>prelu</em> or <em>elu</em> or <em>leakyrelu</em> or <em>selu</em></p></li>
</ul>
</li>
</ul>
<dl>
<dt><strong>Example:</strong></dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">:</span>
  <span class="nb">type</span><span class="p">:</span> <span class="n">mlp</span>
  <span class="n">params</span><span class="p">:</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="mi">512</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="mf">0.2</span>
    <span class="n">num_hidden_layers</span><span class="p">:</span> <span class="mi">5</span>
    <span class="n">shrink_rate</span><span class="p">:</span> <span class="mf">0.5</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">prelu</span>
</pre></div>
</div>
</dd>
</dl>
</section>
<section id="cnn">
<h2>CNN<a class="headerlink" href="#cnn" title="Permalink to this heading"></a></h2>
<p>Convolutional Neural Network (CNN) is a type of deep neural network commonly
used for image analysis.
The CNN model implemented in the XANESNET includes an input layer,
one or more convolutional layers, two dense layers, and an output layer.
Each layer plays a role in extracting and transforming features from the input data.</p>
<p>Each convolutional layer comprises a 1D convolution layer,
batch normalisation, activation function, and a dropout layer.
The number of output channels for the initial convolutional layer is defined
in the input file, and for each subsequent convolutional layer,
the channel number is increased multiplicatively.</p>
<p>For the two dense layers, the first layer includes a linear layer, a dropout rate, and an activation function.
The second dense layer consists solely of a linear layer.</p>
<p><strong>Input file:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">type:</span> <span class="pre">cnn</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">params</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_size</span></code> (int): The size of the initial hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dropout</span></code> (float):  Dropout probability for the hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_hidden_layers</span></code> (int): Number of hidden layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shrink_rate</span></code> (float): Rate to reduce the hidden layer size multiplicatively.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">activation</span></code> (str): Type of activation function. Option: <em>relu</em> or <em>sigmoid</em> or <em>tanh</em> or <em>prelu</em> or <em>elu</em> or <em>leakyrelu</em> or <em>selu</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">out_channel</span></code> (int): Number of output channels for the initial convolutional layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">channel_mul</span></code> (int): Channel multiplication factor for increasing output channels in subsequent convolutional layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> (int): The size of the convolutional kernel</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stride</span></code> (int): Stride of the convolution operation.</p></li>
</ul>
</li>
</ul>
<dl>
<dt><strong>Example:</strong></dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">:</span>
  <span class="nb">type</span><span class="p">:</span> <span class="n">cnn</span>
  <span class="n">params</span><span class="p">:</span>
      <span class="n">hidden_size</span><span class="p">:</span> <span class="mi">64</span>
      <span class="n">dropout</span><span class="p">:</span> <span class="mf">0.2</span>
      <span class="n">num_conv_layers</span><span class="p">:</span> <span class="mi">3</span>
      <span class="n">activation</span><span class="p">:</span> <span class="n">prelu</span>
      <span class="n">out_channel</span><span class="p">:</span> <span class="mi">32</span>
      <span class="n">channel_mul</span><span class="p">:</span> <span class="mi">2</span>
      <span class="n">kernel_size</span><span class="p">:</span> <span class="mi">3</span>
      <span class="n">stride</span><span class="p">:</span> <span class="mi">1</span>
</pre></div>
</div>
</dd>
</dl>
</section>
<section id="lstm">
<h2>LSTM<a class="headerlink" href="#lstm" title="Permalink to this heading"></a></h2>
<p>Long Short-Term Memory Networks (LSTM) is special type of recurrent neural network
with additional memory cells to handles long-term dependencies in sequence data.
The LSTM model implemented in XANESNET consists of a bidirectional LSTM layer follow by a
feedforward neural network with two dense layers.</p>
<p>The defined LSTM is bidirectional, processing the input sequence in both
forward and backward directions. The output size of the LSTM is
2 x the number of features in the LSTM hidden state.
The intermediate hidden size of the following FNN is user-specified.</p>
<p><strong>Input file:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">type:</span> <span class="pre">lstm</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">params</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_size</span></code> (int): The size of LSTM hidden state.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_out_size</span></code> (float):  The output size of intermediate hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_layers</span></code> (int): Number of hidden layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dropout</span></code> (float): Dropout probability for the hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">activation</span></code> (str): Type of activation function. Option: <em>relu</em> or <em>sigmoid</em> or <em>tanh</em> or <em>prelu</em> or <em>elu</em> or <em>leakyrelu</em> or <em>selu</em></p></li>
</ul>
</li>
</ul>
<dl>
<dt><strong>Example:</strong></dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">:</span>
   <span class="nb">type</span><span class="p">:</span> <span class="n">lstm</span>
   <span class="n">params</span><span class="p">:</span>
     <span class="n">hidden_size</span><span class="p">:</span> <span class="mi">100</span>
     <span class="n">hidden_out_size</span><span class="p">:</span> <span class="mi">50</span>
     <span class="n">num_layers</span><span class="p">:</span> <span class="mi">1</span>
     <span class="n">dropout</span><span class="p">:</span> <span class="mf">0.2</span>
     <span class="n">activation</span><span class="p">:</span> <span class="n">prelu</span>
</pre></div>
</div>
</dd>
</dl>
</section>
<section id="ae-mlp">
<h2>AE-MLP<a class="headerlink" href="#ae-mlp" title="Permalink to this heading"></a></h2>
<p>Autoencoder Multilayer Perceptron (AE-MLP) is a type of deep neural network
used for unsupervised learning of data encodings.
In XANESNET, the architecture of AE-MLP includes three main parts: encoder, decoder, and dense layers.</p>
<p>The encoder is constructed using one or more hidden layers, with each layer
consists of a linear layer and activation function.
The size of each hidden layer decreases multiplicatively
based on the user-specified value <code class="docutils literal notranslate"><span class="pre">shrink_rate</span></code>. The decoder has a
similar structure with the size of each hidden layer instead
increasing multiplicatively.
The dense layers that perform prediction consist of linear layers.
The first dense layer also include activation function and a dropout layer.</p>
<p>The AE-MLP model can reconstruct the input data
as well as predict the output data.
Reconstruction performs a forward pass through the encoder and decoder.
Prediction performs a forward pass through the encoder and dense layers.</p>
<p><strong>Input file:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">type:</span> <span class="pre">ae_mlp</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">params</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_size</span></code> (int): The size of the initial hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dropout</span></code> (float):  Dropout probability for the hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_hidden_layers</span></code> (int): Number of hidden layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shrink_rate</span></code> (float): Rate to change the hidden layer size multiplicatively.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">activation</span></code> (str): Type of activation function. Option: <em>relu</em> or <em>sigmoid</em> or <em>tanh</em> or <em>prelu</em> or <em>elu</em> or <em>leakyrelu</em> or <em>selu</em></p></li>
</ul>
</li>
</ul>
<dl>
<dt><strong>Example:</strong></dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">:</span>
  <span class="nb">type</span><span class="p">:</span> <span class="n">ae_mlp</span>
  <span class="n">params</span><span class="p">:</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="mi">512</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="mf">0.2</span>
    <span class="n">num_hidden_layers</span><span class="p">:</span> <span class="mi">5</span>
    <span class="n">shrink_rate</span><span class="p">:</span> <span class="mf">0.5</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">prelu</span>
</pre></div>
</div>
</dd>
</dl>
</section>
<section id="ae-cnn">
<h2>AE-CNN<a class="headerlink" href="#ae-cnn" title="Permalink to this heading"></a></h2>
<p>Autoencoder Multilayer Perceptron (AE-MLP) is a type of deep neural network
used for unsupervised learning of data encodings.
In XANESNET, the architecture of AE-CNN includes three main parts: encoder, decoder, and dense layers.</p>
<p>The encoder is constructed using one or more hidden layers, with each layer
consists of a 1D convolutional layer and activation function.
The number of output channels for the initial convolutional layer is defined
in the input file, and for each subsequent convolutional layer,
the channel number is increased multiplicatively.
The decoder is constructed as a sequence of transpose convolutional layers
with dimensions matching the corresponding layers in the encoder.
The dense layers that perform prediction consist of linear layers.
The first dense layer also include activation function and a dropout layer.</p>
<p>The AE-CNN model can reconstruct the input data
as well as predict the output data.
Reconstruction performs a forward pass through the encoder and decoder.
Prediction performs a forward pass through the encoder and dense layers.</p>
<p><strong>Input file:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">type:</span> <span class="pre">ae_cnn</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">params</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_size</span></code> (int): The size of the initial hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dropout</span></code> (float):  Dropout probability for the hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_hidden_layers</span></code> (int): Number of hidden layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shrink_rate</span></code> (float): Rate to reduce the hidden layer size multiplicatively.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">activation</span></code> (str): Type of activation function. Option: <em>relu</em> or <em>sigmoid</em> or <em>tanh</em> or <em>prelu</em> or <em>elu</em> or <em>leakyrelu</em> or <em>selu</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">out_channel</span></code> (int): Number of output channels for the initial convolutional layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">channel_mul</span></code> (int): Channel multiplication factor for increasing output channels in subsequent convolutional layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> (int): The size of the convolutional kernel</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stride</span></code> (int): Stride of the convolution operation.</p></li>
</ul>
</li>
</ul>
<dl>
<dt><strong>Example:</strong></dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">:</span>
  <span class="nb">type</span><span class="p">:</span> <span class="n">ae_cnn</span>
  <span class="n">params</span><span class="p">:</span>
      <span class="n">hidden_size</span><span class="p">:</span> <span class="mi">64</span>
      <span class="n">dropout</span><span class="p">:</span> <span class="mf">0.2</span>
      <span class="n">num_conv_layers</span><span class="p">:</span> <span class="mi">3</span>
      <span class="n">activation</span><span class="p">:</span> <span class="n">prelu</span>
      <span class="n">out_channel</span><span class="p">:</span> <span class="mi">32</span>
      <span class="n">channel_mul</span><span class="p">:</span> <span class="mi">2</span>
      <span class="n">kernel_size</span><span class="p">:</span> <span class="mi">3</span>
      <span class="n">stride</span><span class="p">:</span> <span class="mi">1</span>
</pre></div>
</div>
</dd>
</dl>
</section>
<section id="aegan-mlp">
<h2>AEGAN-MLP<a class="headerlink" href="#aegan-mlp" title="Permalink to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">aegan_mlp</span></code> model trains both structure and spectra at the same time
using two autoencoders or generators that have shared parameters and two discriminators.
The discriminators are used to encourage better performance from the generators,
whilst the generators try to fool the discriminators.
This structure allows different paths through the model for either data type.
It can be used to reconstruct the input data or predict the output data
for either structure or spectra without changing the model.
All constituent parts of the model are multilayer perceptron networks.
Aside from input and output shape,
the dimension of the linear layers is currently fixed.
The generative and discriminative parts of the model can have different loss functions,
learning rates and optimisers.</p>
<p>Training of the AEGAN is achieved as alternating updates of the generative
and discriminative parts of the model.
The loss for the generative part is calculated as the sum of the
scaled difference between model output and target output for both
reconstructions and predictions for spectra and structure.
The individual losses are currently scaled by the max value of the model output
to compensate for the scaling differences between spectra and structure.</p>
<p>The discriminator part of the model tries to predict whether the data is real -
i.e. from the training set - or fake - produced from the generative part of the model.
The total loss is the sum of the <em>real loss</em> and <em>fake loss</em>.
The fake loss is calculated as the difference between the predicted labels
for the real and fake data produced by the generator.
The real loss is calculated as the the difference between the predicted labels
for the fake data and the true label for the data.</p>
<p><strong>Network Diagram:</strong></p>
<img alt="_images/model_aegan_diagram.png" class="align-center" src="_images/model_aegan_diagram.png" />
<p><strong>Input file:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">type:</span> <span class="pre">aegan_mlp</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">params</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_size</span></code> (int): The size of the initial hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dropout</span></code> (float):  Dropout probability for the hidden layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_hl_gen</span></code> (int): Number of hidden layers for structure encoder &amp; spectrum encoder in generative part.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_hl_shared</span></code> (int): Number of hidden layers for shared encoder &amp; shared decoder in generative part.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_hl_dis</span></code> (int):  Number of hidden layers for discriminative part</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">activation</span></code> (str): Type of activation function. Option: <em>relu</em> or <em>sigmoid</em> or <em>tanh</em> or <em>prelu</em> or <em>elu</em> or <em>leakyrelu</em> or <em>selu</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lr_gen</span></code> (float): Learning rate for generative part.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lr_dis</span></code> (float): Learning rate for discriminative part.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optim_fn_gen</span></code> (str): Type of optimisation function for generative part. Options: <em>Adam</em> or <em>SGD</em> or <em>RMSprop</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optim_fn_dis</span></code> (str): Type of optimisation function for discriminative part. Options: <em>Adam</em> or <em>SGD</em> or <em>RMSprop</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_gen</span></code>: Loss function for generative part.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_fn</span></code> (str): Type of loss function. Options: <em>mse</em> or <em>bce</em> or <em>emd</em> or <em>cosine</em> or <em>l1</em> or <em>wcc</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_args</span></code> (float or ‘null’): Additional arguments for the loss function if needed.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_reg_type</span></code> (str): regularisation type. Options: <em>null</em> or <em>L1</em> or <em>L2</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_reg_param</span></code> (float): strength of regularisation.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_dis</span></code>: Loss function for discriminative part.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_fn</span></code> (str): Type of loss function. Options: <em>mse</em> or <em>bce</em> or <em>emd</em> or <em>cosine</em> or <em>l1</em> or <em>wcc</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_args</span></code> (float or ‘null’): Additional arguments for the loss function if needed.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_reg_type</span></code> (str): regularisation type. Options: <em>null</em> or <em>L1</em> or <em>L2</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_reg_param</span></code> (float): strength of regularisation.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<dl>
<dt><strong>Example:</strong></dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">:</span>
  <span class="nb">type</span><span class="p">:</span> <span class="n">aegan_mlp</span>
  <span class="n">params</span><span class="p">:</span>
      <span class="n">hidden_size</span><span class="p">:</span> <span class="mi">256</span>
      <span class="n">dropout</span><span class="p">:</span> <span class="mf">0.0</span>
      <span class="n">n_hl_gen</span><span class="p">:</span> <span class="mi">2</span>
      <span class="n">n_hl_shared</span><span class="p">:</span> <span class="mi">2</span>
      <span class="n">n_hl_dis</span><span class="p">:</span> <span class="mi">2</span>
      <span class="n">activation</span><span class="p">:</span> <span class="n">prelu</span>
      <span class="n">lr_gen</span><span class="p">:</span> <span class="mf">0.01</span>
      <span class="n">lr_dis</span><span class="p">:</span> <span class="mf">0.0001</span>
      <span class="n">optim_fn_gen</span><span class="p">:</span> <span class="n">Adam</span>
      <span class="n">optim_fn_dis</span><span class="p">:</span> <span class="n">Adam</span>
      <span class="n">loss_gen</span><span class="p">:</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">mse</span>
        <span class="n">loss_args</span><span class="p">:</span> <span class="mi">10</span>
        <span class="n">loss_reg_type</span><span class="p">:</span> <span class="n">null</span>
        <span class="n">loss_reg_param</span><span class="p">:</span> <span class="mf">0.001</span>
      <span class="n">loss_dis</span><span class="p">:</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">bce</span>
        <span class="n">loss_args</span><span class="p">:</span> <span class="n">null</span>
        <span class="n">loss_reg_type</span><span class="p">:</span> <span class="n">null</span>
        <span class="n">loss_reg_param</span><span class="p">:</span> <span class="mf">0.001</span>
</pre></div>
</div>
</dd>
</dl>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="descriptor.html" class="btn btn-neutral float-left" title="Library of Descriptors" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="evaluation.html" class="btn btn-neutral float-right" title="Model Evaluation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Newcastle University.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>