x_path:  data/datasets/fe/preconv/xyz_train
y_path:  data/datasets/fe/preconv/xanes_train

descriptor:
    type: wacsf
    params:
      r_min: 1.0
      r_max: 6.0
      n_g2: 16
      n_g4: 32

load_guess: False
loadguess_params:
  model_dir: ./model/model_001/model.pt
  freeze_mlp_params:    # MLP Freeze Layer Parameters:
    freeze_dense: True
    n_freeze_dense: 2
  freeze_ae_mlp_params: # AE_MLP Freeze Layer Parameters:
    freeze_encoder: True
    freeze_decoder: True
    freeze_dense: True
    n_freeze_encoder: 2
    n_freeze_decoder: 2
    n_freeze_dense: 1

bootstrap: False 
bootstrap_params: 
  n_boot: 3
  n_size: 1.0
  seed_boot: [97, 39, 22]

ensemble: False
ensemble_params:
  n_ens: 3
  weight_init_seed: [97, 39, 22]

data_params: False
augment:
  augment_type: random_noise
  # augment_type: random_combination
  augment_mult: 5
  # for random noise
  augment_params: 
    normal_mean: 0
    normal_sd: 0.1
  # for random combination
  # augment_params : {}

kfold: False
kfold_params:
  n_splits: 5
  n_repeats: 1
  loss: 
    loss_fn: mse 
    loss_args: null
    loss_reg_type: L2 # null, L1, L2
    loss_reg_param: 0.001

lr_scheduler: 
  scheduler: True
  scheduler_type: step
  scheduler_param:
    step_size: 100
    gamma: 0.5
          
hyperparams: 
  model: mlp
  batch_size: 64
  n_hl: 5
  hl_ini_dim: 512
  hl_shrink: 0.5
  activation: prelu
  loss:
    loss_fn: mse
    loss_args: null
    loss_reg_type: L2 # null, L1, L2
    loss_reg_param: 0.001
  lr: 0.0001
  optim_fn: Adam
  dropout: 0.2
  weight_init_seed: 2023
  kernel_init: xavier_uniform
  bias_init: zeros

seed: 2021
epochs: 1
variance_threshold: 0.0
model_eval : False

optuna_params:
  tune: False
  n_trials: 3
  tune_optim_fn: True
  tune_batch_size: True
  tune_activation: True
  tune_loss_fn: True
  tune_lr: True
  tune_dropout: True
  tune_hidden_layers: True
  